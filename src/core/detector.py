import cv2
import mediapipe as mp
import numpy as np
import yaml
import os
from collections import deque
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

class FaceDetector:
    """
    æ¡ç”¨ MediaPipe æœ€æ–° Tasks API çš„åµæ¸¬å™¨ã€‚
    æ•´åˆ 3D çµæ§‹è¦–å·®èˆ‡çœ¼éƒ¨å¾®éœ‡é¡«æª¢æ¸¬ï¼Œåš´æ ¼é˜²ç¯„å½±ç‰‡èˆ‡ç…§ç‰‡æ”»æ“Šã€‚
    """
    def __init__(self, config_path="config.yaml"):
        # è¼‰å…¥è¨­å®š
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è®€å–é…ç½®åƒæ•¸
        model_path = self.config.get('database', {}).get('model_path', 'models/face_landmarker.task')
        det_confidence = self.config.get('thresholds', {}).get('detection_confidence', 0.6)
        liveness_score_threshold = self.config.get('thresholds', {}).get('liveness_score', 1.0)
        
        # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
            print("ğŸ’¡ è«‹ä¸‹è¼‰ face_landmarker.task ä¸¦æ”¾è‡³ models/ è³‡æ–™å¤¾ã€‚")
            # å»ºç«‹ç›®éŒ„ä»¥æ–¹ä¾¿ä½¿ç”¨è€…æ”¾ç½®
            os.makedirs(os.path.dirname(model_path), exist_ok=True)

        # 1. MediaPipe Tasks é¸é …è¨­å®š
        base_options = python.BaseOptions(model_asset_path=model_path)
        options = vision.FaceLandmarkerOptions(
            base_options=base_options,
            running_mode=vision.RunningMode.IMAGE, # ç‚ºäº†æ–¹ä¾¿æ•´åˆ OpenCV å¾ªç’°ï¼Œä½¿ç”¨ IMAGE æ¨¡å¼
            num_faces=2,  # è¨­ç‚º 2 ä»¥åµæ¸¬å¤šäººå¹²æ“¾
            min_face_detection_confidence=det_confidence,
            output_face_blendshapes=True,
            output_facial_transformation_matrixes=True
        )
        
        # å»ºç«‹åµæ¸¬å™¨
        self.landmarker = vision.FaceLandmarker.create_from_options(options)
        
        # æ´»é«”æª¢æ¸¬åƒæ•¸
        self.liveness_score = 0.0
        self.liveness_threshold = liveness_score_threshold
        self.history_landmarks = deque(maxlen=20)
        
        # é—œéµé»ç´¢å¼• (Tasks API çš„ç´¢å¼•èˆ‡ Face Mesh ç›¸åŒ)
        self.NOSE_TIP = 1 

    def _check_3d_parallax(self, landmarks):
        """æ ¸å¿ƒé˜²å½ï¼š3D è¦–å·®æª¢æŸ¥"""
        if len(self.history_landmarks) < 5:
            return 0.0
        
        # å–å¾—é¼»å°–åº§æ¨™
        curr_nose = np.array([landmarks[self.NOSE_TIP].x, landmarks[self.NOSE_TIP].y, landmarks[self.NOSE_TIP].z])
        prev_nose = self.history_landmarks[-2][self.NOSE_TIP]
        
        # é¼»å°–å¹³é¢ä½ç§»
        nose_move = np.linalg.norm(curr_nose[:2] - prev_nose[:2])
        if nose_move < 0.001: return -0.05 # éœæ­¢æ‰£åˆ†

        # è¨ˆç®—ä¸åŒæ·±åº¦é»çš„ä½ç§»æ¯”æ¨™æº–å·®
        depth_changes = []
        for idx in [33, 263, 152, 10]:
            curr_pt = np.array([landmarks[idx].x, landmarks[idx].y])
            prev_pt = self.history_landmarks[-2][idx][:2]
            pt_move = np.linalg.norm(curr_pt - prev_pt)
            if pt_move > 0:
                depth_changes.append(nose_move / pt_move)
        
        if not depth_changes: return 0.0

        std_val = np.std(depth_changes)
        if 0.002 < std_val < 0.08: return 0.15 # çœŸäººç‰¹å¾µ
        return 0.0

    def _calculate_ear(self, landmarks, w, h):
        """è¨ˆç®—å¹³å‡çœ¼ç›å¤–è§€æ¯”ä¾‹ (EAR)"""
        def get_ear(indices):
            # Tasks API çš„é»ç›´æ¥å…·å‚™ x, y å±¬æ€§
            pts = [np.array([landmarks[i].x * w, landmarks[i].y * h]) for i in indices]
            v = np.linalg.norm(pts[1] - pts[5]) + np.linalg.norm(pts[2] - pts[4])
            h_dist = np.linalg.norm(pts[0] - pts[3])
            return v / (2.0 * h_dist)
        # å·¦çœ¼èˆ‡å³çœ¼ç´¢å¼•
        l_ear = get_ear([362, 385, 387, 263, 373, 380])
        r_ear = get_ear([33, 160, 158, 133, 153, 144])
        return (l_ear + r_ear) / 2.0

    def process(self, frame):
        """
        è™•ç†å½±åƒå¹€ã€‚
        :param frame: OpenCV æ ¼å¼å½±åƒ (BGR)
        :return: (status, data)
        """
        h, w, _ = frame.shape
        
        # è½‰æ›ç‚º MediaPipe Image æ ¼å¼
        # æ³¨æ„ï¼šTasks API è™•ç†çš„æ˜¯ RGB æ ¼å¼
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
        
        # åŸ·è¡Œåµæ¸¬
        result = self.landmarker.detect(mp_image)
        
        if not result.face_landmarks:
            self.reset_liveness()
            return "NO_FACE", None
        
        if len(result.face_landmarks) > 1:
            return "MULTIPLE_FACES", None
            
        # å–å¾—ç¬¬ä¸€å¼µè‡‰çš„é»ä½
        landmarks = result.face_landmarks[0]
        
        # æ›´æ–°æ­·å²ç´€éŒ„ (å°‡é»ä½ç‰©ä»¶è½‰ç‚º numpy é™£åˆ—ä»¥ä¾¿è¨ˆç®—)
        curr_pts = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])
        self.history_landmarks.append(curr_pts)

        # --- æ´»é«”æª¢æ¸¬ç´¯ç© ---
        # 1. 3D è¦–å·®
        self.liveness_score += self._check_3d_parallax(landmarks)
        
        # 2. çœ¼éƒ¨å¾®å‹• (EAR)
        ear = self._calculate_ear(landmarks, w, h)
        if 0.1 < ear < 0.22:
            self.liveness_score += 0.1
            
        # åˆ†æ•¸æ§åˆ¶
        self.liveness_score = max(0.0, min(self.liveness_score, 1.2))
        is_live = self.liveness_score >= self.liveness_threshold

        # è¨ˆç®— BBox (åŸºæ–¼é‚Šç•Œé»)
        x_coords = [lm.x for lm in landmarks]
        y_coords = [lm.y for lm in landmarks]
        bbox = [int(min(x_coords)*w), int(min(y_coords)*h), int(max(x_coords)*w), int(max(y_coords)*h)]

        return "SUCCESS", {
            "bbox": bbox,
            "is_live": is_live,
            "liveness_percent": min(int(self.liveness_score * 100), 100)
        }

    def reset_liveness(self):
        self.liveness_score = 0.0
        self.history_landmarks.clear()

    def __del__(self):
        """é—œé–‰åµæ¸¬å™¨é‡‹æ”¾è³‡æº"""
        if hasattr(self, 'landmarker'):
            self.landmarker.close()

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    cap = cv2.VideoCapture(0)
    detector = FaceDetector()
    
    print("ğŸ¬ é–‹å§‹æ¸¬è©¦ Tasks API åµæ¸¬å™¨ (æŒ‰ 'q' é€€å‡º)...")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        
        status, res = detector.process(frame)
        
        if status == "SUCCESS":
            color = (0, 255, 0) if res['is_live'] else (0, 165, 255)
            cv2.rectangle(frame, (res['bbox'][0], res['bbox'][1]), (res['bbox'][2], res['bbox'][3]), color, 2)
            cv2.putText(frame, f"Tasks 3D Liveness: {res['liveness_percent']}%", (10, 30), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        else:
            cv2.putText(frame, f"Status: {status}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            
        cv2.imshow("MediaPipe Tasks Detector", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'): break
    
    cap.release()
    cv2.destroyAllWindows()